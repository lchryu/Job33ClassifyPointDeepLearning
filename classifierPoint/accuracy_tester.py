"""
Module Ä‘Æ¡n giáº£n Ä‘á»ƒ test accuracy báº±ng cÃ¡ch so sÃ¡nh:
- Folder chá»©a .LiData gá»‘c (cÃ³ ground truth labels)  
- Folder chá»©a .LiData Ä‘Ã£ Ä‘Æ°á»£c model phÃ¢n lá»›p (predictions)
"""

import numpy as np
import gvlib
from pathlib import Path
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


def read_lidata_labels(lidata_path):
    """Äá»c file LiData vÃ  chá»‰ tráº£ vá» classification labels"""
    try:
        reader = gvlib.LidataReader(str(lidata_path))
        if not reader.open():
            raise IOError(f"KhÃ´ng thá»ƒ má»Ÿ file {lidata_path}")
        
        reader.read()
        lidata = reader.tile()
        
        # Chá»‰ láº¥y classification
        classification = lidata.classification
        return classification
        
    except Exception as e:
        print(f"Lá»—i khi Ä‘á»c file {lidata_path}: {str(e)}")
        raise


def calculate_accuracy(ground_truth_folder: str, predictions_folder: str) -> dict:
    """
    TÃ­nh accuracy báº±ng cÃ¡ch stack táº¥t cáº£ labels tá»« nhiá»u file
    
    Args:
        ground_truth_folder: Folder chá»©a file .LiData gá»‘c (cÃ³ labels tháº­t)
        predictions_folder: Folder chá»©a file .LiData Ä‘Ã£ Ä‘Æ°á»£c model predict
    
    Returns:
        dict: Káº¿t quáº£ accuracy vÃ  thá»‘ng kÃª
    """
    
    ground_truth_path = Path(ground_truth_folder)
    predictions_path = Path(predictions_folder)
    
    # TÃ¬m táº¥t cáº£ file .LiData
    gt_files = sorted(list(ground_truth_path.glob("*.LiData")))
    
    print(f"TÃ¬m tháº¥y {len(gt_files)} file .LiData")
    
    if len(gt_files) == 0:
        raise ValueError("KhÃ´ng tÃ¬m tháº¥y file .LiData trong folder ground truth")
    
    # Lists Ä‘á»ƒ stack táº¥t cáº£ labels
    all_y_true = []
    all_y_pred = []
    file_stats = []
    
    print("Äang xá»­ lÃ½ cÃ¡c file...")
    print("-" * 50)
    
    # Láº·p qua tá»«ng file vÃ  stack labels
    for i, gt_file in enumerate(gt_files):
        # TÃ¬m file prediction tÆ°Æ¡ng á»©ng
        pred_file = predictions_path / gt_file.name
        
        if not pred_file.exists():
            print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file prediction: {gt_file.name}")
            continue
        
        print(f"ğŸ“ [{i+1}/{len(gt_files)}] {gt_file.name}")
        
        try:
            # Äá»c labels
            y_true = read_lidata_labels(gt_file)
            y_pred = read_lidata_labels(pred_file)
            
            # Kiá»ƒm tra sá»‘ Ä‘iá»ƒm
            if len(y_true) != len(y_pred):
                print(f"   âš ï¸  Sá»‘ Ä‘iá»ƒm khÃ´ng khá»›p: GT={len(y_true):,}, Pred={len(y_pred):,}")
                # Láº¥y sá»‘ Ä‘iá»ƒm nhá» hÆ¡n
                min_len = min(len(y_true), len(y_pred))
                y_true = y_true[:min_len]
                y_pred = y_pred[:min_len]
            
            # Stack vÃ o lists tá»•ng
            all_y_true.extend(y_true)
            all_y_pred.extend(y_pred)
            
            # LÆ°u thá»‘ng kÃª file
            file_stats.append({
                'filename': gt_file.name,
                'num_points': len(y_true),
                'unique_gt_classes': len(np.unique(y_true)),
                'unique_pred_classes': len(np.unique(y_pred))
            })
            
            print(f"   âœ… {len(y_true):,} points")
            
        except Exception as e:
            print(f"   âŒ Lá»—i: {e}")
            continue
    
    if len(all_y_true) == 0: raise ValueError("KhÃ´ng cÃ³ dá»¯ liá»‡u nÃ o Ä‘Æ°á»£c xá»­ lÃ½ thÃ nh cÃ´ng")
    
    print("-" * 50)
    print("ğŸ”„ Äang tÃ­nh metrics...")
    
    # Convert to numpy arrays
    y_true_array = np.array(all_y_true)
    y_pred_array = np.array(all_y_pred)
    
    # TÃ­nh cÃ¡c metrics
    overall_accuracy = accuracy_score(y_true_array, y_pred_array)
    # TÃ­nh precision, recall, f1 vá»›i average='weighted' Ä‘á»ƒ xá»­ lÃ½ multi-class
    precision = precision_score(y_true_array, y_pred_array, average='weighted', zero_division=0)
    recall = recall_score(y_true_array, y_pred_array, average='weighted', zero_division=0)
    f1 = f1_score(y_true_array, y_pred_array, average='weighted', zero_division=0)
    
    # Chá»‰ lÆ°u thÃ´ng tin cáº§n thiáº¿t
    dicts_results = {
        'overall_accuracy': overall_accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'total_points': len(y_true_array),
        'files_processed': len(file_stats),
    }
    
    return dicts_results

def print_results(results):
    print("\n=== Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ ===")
    print(f"ğŸ¯ Accuracy: {results['overall_accuracy']:.4f} ({results['overall_accuracy']*100:.2f}%)")
    print(f"ğŸ“ Precision: {results['precision']:.4f} ({results['precision']*100:.2f}%)")
    print(f"ğŸ“ Recall: {results['recall']:.4f} ({results['recall']*100:.2f}%)")
    print(f"ğŸ“Š F1-score: {results['f1_score']:.4f} ({results['f1_score']*100:.2f}%)")
    print(f"ğŸ“ˆ Total points: {results['total_points']:,}")


# Main execution
if __name__ == "__main__":
    ground_truth_folder = "D:\DATA_QA\QA_test"                      # Folder chá»©a file gá»‘c cÃ³ labels
    predictions_folder = "D:\DATA_QA\QA_test_classify"              # Folder chá»©a file Ä‘Ã£ predict
    
    # TÃ­nh accuracy
    print_results(calculate_accuracy(ground_truth_folder, predictions_folder))